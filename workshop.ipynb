{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Science Society Workshop 6: Scikit-Learn (Linear Regression)\n",
    "In this workshop we begin our work on Scikit-Learn. This Python package is one of the most popular machine learning packages for Python. Our workshop today will focus on linear regression.\n",
    "### Installation of Sci-Kit Learn\n",
    "You may not have Sci-Kit Learn installed on your device. Run the following code cell to install Sci-Kit Learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sklearn\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/7a/dbb3be0ce9bd5c8b7e3d87328e79063f8b263b2b1bfa4774cb1147bfcd3f/sklearn-0.0.tar.gz\n",
      "Requirement already satisfied: scikit-learn in c:\\program files\\anaconda3\\lib\\site-packages (from sklearn) (0.20.3)\n",
      "Requirement already satisfied: numpy>=1.8.2 in c:\\program files\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Requirement already satisfied: scipy>=0.13.3 in c:\\program files\\anaconda3\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Building wheels for collected packages: sklearn\n",
      "  Building wheel for sklearn (setup.py): started\n",
      "  Building wheel for sklearn (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\zcaposm\\AppData\\Local\\pip\\Cache\\wheels\\76\\03\\bb\\589d421d27431bcd2c6da284d5f2286c8e3b2ea3cf1594c074\n",
      "Successfully built sklearn\n",
      "Installing collected packages: sklearn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [WinError 5] Access is denied: 'C:\\\\Program Files\\\\Anaconda3\\\\Lib\\\\site-packages\\\\sklearn-0.0.dist-info'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing\n",
    "There are a number of modules within sklearn so when importing functions often we'll need to import from the relevant module. Run the code cell below to import everything we'll need for the workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Problem\n",
    "Below there are two code cells to extract a set of data and then plot it. The first code cell generates a data frame by reading the csv file \"RegressionData.csv\". Plotting the two arrays reveals a set of data that seems to show some kind of relationship between ````x```` and ````y````. The data here seems to change linearly as ````x```` changes. We can model this with a line of best fit for the data. The equation for a straight line is given by:\n",
    "$$y = mx + c $$\n",
    "Where $y$ is known as your label or dependant variable and $x$ is your feature or independant variable. The value $m$ is the gradient or the slope of the line, while $c$ is the $y$ intercept ie the point where the line crosses the $y$-axis. The algorithms we will use today from Sci-Kit Learn are designed to determine the values of $m$ and $c$ so that a line of best fit may be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates data frame from csv file\n",
    "df = pd.read_csv(\"RegressionData.csv\")\n",
    "\n",
    "# Turning the columns into arrays\n",
    "x = df[\"x\"].values\n",
    "y = df[\"y\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plots the data from the above data\n",
    "plt.figure()\n",
    "plt.grid(True)\n",
    "plt.plot(x,y,'r.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "#### Reshaping\n",
    "The first step as is to prepare the data. This step is known as preprocessing. The first thing that's necessary in this case is to reshape the data. This is because our data is currently in the form of an array that is just one long row when the functions from Sci-Kit Learn deal with columns. To fix this we can use ````np.reshape(x,(-1,1))````. Here we will use ````x.reshape(-1,1)```` which for a Numpy array ````x```` is equivalent to using the ````np.reshape```` function as described earlier. Here the $1$ specifys that the data will have one column and $-1$ allocates a value for this dimension based on how many dimensions are left. Since here we have $1$ column the data can only be shaped as one column. It is also worth noting that we typically use a capital ````X```` to denote our features or independent variables and again we use ````y```` for our dependent variable or label. Depending on how your data is formatted to begin with this step may not be required. If your ````X```` is a data frame with multiple columns for instance you will not need to convert it to an array. Anything with one column likely needs to be converted to an array and reshaped as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original shape of x and y\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Independant variable or features\n",
    "X = x.reshape(-1,1)\n",
    "\n",
    "# Dependant variable or labels\n",
    "y = y.reshape(-1,1)\n",
    "\n",
    "# Reshaped X and y\n",
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Seperating into Training and Test Sets\n",
    "Here we split our data into training sets and test sets for each variable using the ````train_test_split```` function. The input is as you would expect your two variables and an additional argument the ratio at which the data is split. Here we passed ````test_size=0.2```` so $20\\%$ of the data is allocated to the test set while $80\\%$ is allocated to the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperates the data into test and training sets \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Regressor\n",
    "We begin by defining a ````regressor```` by calling the ````LinearRegression```` function. The argument ````n_jobs```` specifys how much processing power we want to dedicate to the regressor. The higher the value given to this argument the faster the classifier will run. If you pass $-1$ as the value for the regressor will use all processers for the job. Then we pass ````regressor.fit(X_train, y_train)````. This passes our training data into the regressor in order to train it. For the linear regression algorithm training it means finding the values of $m$ and $c$ from the equation for the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining our regressor\n",
    "regressor = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Train the regressor\n",
    "fit = regressor.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Returning the Gradient and the Intercept\n",
    "Having trained the regressor we can now return the values of the gradient and intercept. These are simply the attributes ````.coef_```` and ````.intercept_```` of the variable we have used to define our trained regressor. In our case we defined it as ````fit````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns gradient and intercept\n",
    "print(\"Gradient:\",fit.coef_)\n",
    "print(\"Intercept:\",fit.intercept_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line of Best Fit Values\n",
    "Now that we've trained our regressor we can obtain the values for the line of best fit. We do this by passing through a set of points into ````regressor.predict````. For each point the regressor predicts a value based on the line of best fit that was obtained. Essentially, the input is passed through the equation: $ y = mx + c $, where the values of $m$ and $c$ are given by ````fit.coef_```` and ````fit.intercept_```` respectively. The code cell below uses the ````regressor.predict```` to obtain a predicted set of values. Then we obtain another set of values by explicitly plugging into the formula for our line of best fit. We subtract the two sets away from one another to demonstrate that the ````regressor.predict```` function did indeed do the same thing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predicted values \n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# y = m*x + c\n",
    "best_fit_line = fit.coef_*X_test + fit.intercept_\n",
    "\n",
    "# Comparing the two\n",
    "y_pred - best_fit_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the Line of Best Fit\n",
    "Below we plot the line of best fit against the original data. Doing this enables us to make predictions about what ````y```` value we'd obtain for a ````x```` value within the interval covered but not actually recorded in the data set. This is known as interpolation. Our line in effect is a prediction of what the value of ````y```` is as ````x```` changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot of the data with the line of best fit\n",
    "plt.plot(X_test,y_pred)\n",
    "plt.plot(x,y,'r.')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scoring the Model\n",
    "Let us compare the predicted results with the actual values by putting them into a data frame. Note the need for the slicing occurs because ````pd.DataFrame```` takes arrays as rows rather than columns. Since we converted the arrays to columns at the start we need to undo this to convert to a data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converts predicted values and test values to a data frame\n",
    "df = pd.DataFrame({\"Predicted\": y_pred[:,0], \"Actual\": y_test[:,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can score of our model by passing ````regressor.score(X_test,y_test)````. This is data that our regressor hasn't seen yet which means it can provide a basis with which to test the results. The best score you can obtain for the model is $1$. The lower the score the worse a fit the model is to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determines a score for our model\n",
    "score = regressor.score(X_test, y_test)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickling \n",
    "When dealing with large datasets it may take a while to train your regressor (or classifier for a classification algorithm). In such cases its often useful to use pickling; in effect storing the results of the trained model so that it can be loaded in later. To do this we first have to create a new file by using ````open(\"filename\",\"wb\")````. This command tries to open a file with the specified ````\"filename\"````and if no such file is available one will be created. The ````\"wb\"```` argument specifies what we will do with the file. Here the ````w```` indicated we would like to write into the file. We also define our opened file as ````f```` but any suitable variable name is fine. To store the model in the file we write ````pickle.dump(regressor,f)```` where the first argument is the regressor to be stored and the second is the variable name chosen for our file. Once we are done with the file we write ````f.close()```` to close it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates a .pickle file to store our trained regressor\n",
    "f = open(\"linearregression.pickle\",\"wb\")\n",
    "\n",
    "# Stores the regressor in the file f\n",
    "pickle.dump(regressor,f)\n",
    "\n",
    "# Closes the file\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To retrieve our trained regressor we first write ````open(\"linearregression.pickle\",\"rb\")````. Here we swapped ````w```` for ````r```` meaning instead of writing in our file we only want to read it. Then to load in our trained regressor we write ````pickle.load(pickle_in)````. Once again we conclude by closing the file with ````pickle_in.close()````."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opens the file to read\n",
    "pickle_in = open(\"linearregression.pickle\",\"rb\")\n",
    "\n",
    "# Loads the regressor from the file\n",
    "regressor = pickle.load(pickle_in)\n",
    "\n",
    "# Closes the file\n",
    "pickle_in.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then score our pickled regressor and make predictions from it as we would if we had trained it again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring the pickled regressor\n",
    "regressor.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts values of X into line of best fit\n",
    "y_pred = regressor.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plots line of best fit along with the data\n",
    "plt.plot(X,y_pred)\n",
    "plt.plot(x,y,'r.')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Linear Regression\n",
    "The example used above involved only two variables. However, in the real world it is highly unlikely that a label or dependant variable would depend on only one feature or independant variable. The equation for the value of $y$ in such an instance can be generalised as follows:\n",
    "$$ y = c + m_1 x_1 + m_2 x_2 + ... m_n x_n + \\epsilon $$\n",
    "The equation is similar to the one before except we have multiple values for the gradient and we also have an additional $\\epsilon$ term which is the error. In effect the $\\epsilon$ accounts for any potential data points that might not be a fit for the linear model.\n",
    "\n",
    "Below I have an example on how to a linear regression problem with multiple features. The dataset in question is about different types of advertising and the effect on sales. The independent variables are given by the \"TV\", \"Radio\" and \"Newspaper\" columns while \"Sales\" is our dependent variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Converts advertising csv to a data frame\n",
    "df = pd.read_csv(\"advertising.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process for applying the linear regression algorithm when you have multiple independent variables is very similar to that for our original case. The key difference being that ````X```` will consist of more than one column so converting to a Numpy array and reshaping it is not required. Once you have ````X```` and ````y```` the procedure for splitting the data into training and test sets then training the regressor and so forth is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Independent variables\n",
    "X = df.drop(\"Sales\",axis=1)\n",
    "\n",
    "# Dependent variable\n",
    "y = df[\"Sales\"].values.reshape(-1,1)\n",
    "\n",
    "# Splitting into test and training data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining regressor\n",
    "regressor = LinearRegression(n_jobs=-1)\n",
    "\n",
    "# Training our regressor\n",
    "fit = regressor.fit(X_train,y_train)\n",
    "\n",
    "# Predicting values\n",
    "y_pred = fit.predict(X_test)\n",
    "\n",
    "# Scoring our regressor\n",
    "fit.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing predicted against actual values\n",
    "df = pd.DataFrame({\"Predicted\": y_pred[:,0], \"Actual\": y_test[:,0]})\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References \n",
    "[1] advertising.csv : https://www.kaggle.com/ashydv/advertising-dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
